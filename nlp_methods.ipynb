{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Text Classification Models and Vectorization\n",
    "\n",
    "This project compares estimators: Logistic Regression, Decision Tree, and Naive Bayes, and text vectorization strategies: tfidf, and count for a text classification problem.  \n",
    "A pipeline is created is created with one of the estimators, with either tfidt or count vectorization technique.  GridSearchCV in scikitlearn is used to find the best hyper-parameters for the different estimators.  The model pipeline results are generated for each estimator and vectorization combination. \n",
    "\n",
    "### Machine Learning Models\n",
    "#### Logistic Regression\n",
    "Logistic Regression is a linear model used for binary classification tasks, predicting the probability that a given input belongs to a particular class.\n",
    "#### Decision Tree Classifier\n",
    "Decision Tree classifiers split the data into subsets based on the value of input features, creating a tree-like model of decisions. \n",
    "#### Naive Bayes\n",
    "Naive Bayes classifiers are based on Bayesâ€™ theorem, assuming independence between features.\n",
    "\n",
    "### Text Vectorization\n",
    "\n",
    "This is a crucial step in Natural Language Processing (NLP) that involves converting text data into numerical representations, which can then be processed by machine learning algorithms. This project examines and compares the 2 common vectorization techniques:\n",
    "\n",
    "#### Bag of Words (BoW): \n",
    "Also known as Count Vectorization, converts text into numerical features by counting the occurrences of each word in a document. It creates a sparse matrix where each row represents a document and each column represents a word from the vocabulary. \n",
    "\n",
    "#### Term Frequency-Inverse Document Frequency (TF-IDF): \n",
    "This technique not only considers the frequency of words in a document but also their importance across the entire corpus. It reduces the weight of commonly occurring words and gives more importance to less frequent but more meaningful words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "The dataset below is from [kaggle]() and contains a dataset named the \"ColBert Dataset\". (https://arxiv.org/pdf/2004.12765.pdf).  You text column 'humor' is used to classify whether or not the text was humorous.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = pd.read_csv('text_data/dataset-minimal.csv')\n",
    "df = pd.read_csv('text_data/dataset.csv')\n",
    "\n",
    "cvfolds      = 5\n",
    "verbosity    = 0\n",
    "max_features = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "\n",
    "**Text preprocessing:** As a pre-processing step, perform both `stemming` and `lemmatizing` to normalize your text before classifying. For each technique use both the `CountVectorize`r and `TfidifVectorizer` and use options for stop words and max features to prepare the text data for your estimator.\n",
    "\n",
    "**Classification:** Once you have prepared the text data with stemming lemmatizing techniques, consider `LogisticRegression`, `DecisionTreeClassifier`, and `MultinomialNB` as classification algorithms for the data. Compare their performance in terms of accuracy and speed.\n",
    "\n",
    "Share the results of your best classifier in the form of a table with the best version of each estimator, a dictionary of the best parameters and the best score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "models_df = pd.DataFrame({'model': ['Logistic', 'Decision Tree', 'Bayes'], \n",
    "                            'best_params': ['', '', ''],\n",
    "                             'best_score': ['', '', '']}).set_index('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "        'classifier__min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'Bayes': {\n",
    "        'classifier__alpha' : [0.1, 1, 10]\n",
    "    }  # Naive Bayes doesn't have hyperparameters that require tuning in this context\n",
    "}\n",
    "models__ = [\n",
    "  { 'classifier_name': 'Bayes',               'classifier' : MultinomialNB(),         'vectorizer_name' : 'Tfidf', 'vectorizer' : TfidfVectorizer()},\n",
    "  { 'classifier_name': 'Bayes',               'classifier' : MultinomialNB(),         'vectorizer_name' : 'Count', 'vectorizer' : CountVectorizer()},\n",
    "  { 'classifier_name': 'Logistic Regression', 'classifier' : LogisticRegression(),    'vectorizer_name' : 'Tfidf', 'vectorizer' : TfidfVectorizer()},\n",
    "  { 'classifier_name': 'Logistic Regression', 'classifier' : LogisticRegression(),    'vectorizer_name' : 'Count', 'vectorizer' : CountVectorizer()},\n",
    "  { 'classifier_name': 'DecisionTree',        'classifier' : DecisionTreeClassifier(),'vectorizer_name' : 'Tfidf', 'vectorizer' : TfidfVectorizer()},\n",
    "  { 'classifier_name': 'DecisionTree',        'classifier' : DecisionTreeClassifier(),'vectorizer_name' : 'Count', 'vectorizer' : CountVectorizer()},\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stemmer    = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        words = word_tokenize(text)\n",
    "        words = [word for word in words if word not in self.stop_words]\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def stem_text(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "\n",
    "    def lemmatize_text(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return \" \".join(lemmatized_words)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.apply(self.normalize_text).apply(self.stem_text).apply(self.lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def gen_pipeline( vectorizer, classifier ):\n",
    "    pipeline_obj = Pipeline([\n",
    "    ('preprocess', TextPreprocessor()),  # Custom text preprocessing\n",
    "    ('vectorize',  vectorizer ),    # Convert text to TF-IDF vectors\n",
    "    ('classifier', classifier )   # Logistic Regression model\n",
    "    ])\n",
    "    return pipeline_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['humor'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-Vectorization\t\tScore\t\tFit Time\t\tScore Time\n",
      "===================\t\t=====\t\t========\t\t==========\n",
      "Bayes-Tfidf\t0.89\t\t20.67\t\t5.15\n",
      "Bayes-Count\t0.89\t\t23.10\t\t5.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression-Tfidf\t0.89\t\t24.44\t\t5.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression-Count\t0.89\t\t25.29\t\t5.98\n",
      "DecisionTree-Tfidf\t0.59\t\t23.67\t\t5.77\n",
      "DecisionTree-Count\t0.59\t\t23.56\t\t5.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "print(f'Model-Vectorization\\t\\tScore\\t\\tFit Time\\t\\tScore Time')\n",
    "print(f'===================\\t\\t=====\\t\\t========\\t\\t==========')\n",
    "for model in models__:\n",
    "    model_pipeline   = gen_pipeline( model['vectorizer'], model['classifier'] )\n",
    "    model_classifier = GridSearchCV( model_pipeline, param_grids[model['classifier_name']], cv=cvfolds, scoring='accuracy',verbose=verbosity)\n",
    "    model_classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred          = model_classifier.predict(X_test)\n",
    "    gs_accuracy     = accuracy_score(y_test, y_pred)\n",
    "    report_gs_model = classification_report(y_test, y_pred)\n",
    "\n",
    "    fit_time   = np.mean( model_classifier.cv_results_['mean_fit_time']  )\n",
    "    score_time = np.mean( model_classifier.cv_results_['mean_score_time'])\n",
    "    mean_score = np.mean( model_classifier.cv_results_['mean_test_score'])\n",
    "    print(f'{model['classifier_name']}-{model['vectorizer_name']}\\t{mean_score:.2f}\\t\\t{fit_time:.2f}\\t\\t{score_time:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
